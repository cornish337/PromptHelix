# PromptHelix Documentation

This document provides a more detailed overview of the internal structure, components, and advanced testing procedures for PromptHelix. For general information, getting started, setup, and deployment, please refer to the main [README.md](../../README.md) in the root directory.

## Project Structure

For overall project setup, deployment, and basic usage, please refer to the main [README.md](../../README.md).

The core components of PromptHelix are organized as follows:

-   `prompthelix/agents/`: Contains different types of AI agents (Architect, Critic, etc.).
-   `prompthelix/api/`: Defines the FastAPI endpoints and routing.
-   `prompthelix/cli.py`: Command-line interface entry point and commands. See detailed CLI usage below.
-   `prompthelix/config.py`: Application configuration settings. See the main `README.md` for how to configure API keys and database URLs via environment variables.
-   `prompthelix/core/`: Core functionalities like the Prompt DNA system.
-   `prompthelix/database/`: Database models (SQLAlchemy), session management, and initialization.
-   `prompthelix/docs/`: Project documentation, including this file and `agent_specifications.md`.
-   `prompthelix/evaluation/`: Modules for evaluating prompt performance.
-   `prompthelix/evolution/`: Core logic for the genetic algorithm (chromosomes, operators, population), previously `prompthelix/genetics/`.
-   `prompthelix/llm_integrations/`: Modules for interacting with various LLM providers.
-   `prompthelix/main.py`: Main FastAPI application entry point.
-   `prompthelix/schemas/`: Pydantic schemas for data validation and serialization.
-   `prompthelix/services/`: Business logic and services.
-   `prompthelix/tests/`: Unit and integration tests.
    -   `unit/`: Contains unit tests for individual components.
    -   `integration/`: Contains integration tests.
-   `prompthelix/ui/`: HTML templates and static files for the web UI.
-   `prompthelix/utils/`: Utility functions and helpers.
-   `.env.example`: Example environment variable file.
-   `alembic/`: Alembic migration scripts and configuration (if used for DB migrations).
-   `main.py`: (Potentially a root-level script, verify if it's different from `prompthelix/main.py` or if this is a typo in original, assuming `prompthelix/main.py` is the primary one).
-   `requirements.txt`: Project dependencies.
-   `Dockerfile`: Docker configuration for containerization.
-   `CONTRIBUTING.md`: Guidelines for contributing (in root directory).
-   `LICENSE`: Project license information (in root directory).

*Note: The project structure might evolve. Refer to the actual directory layout for the most current information.*

## Implemented Agents

The PromptHelix system is designed around a collection of specialized AI agents that collaborate to generate, refine, and evaluate prompts.

-   **`PromptArchitectAgent`**: Designs the initial genetic structure of prompts based on user requirements, system goals, or existing successful prompt patterns.
-   **`PromptCriticAgent`**: Evaluates and critiques prompts based on their structure, content, and adherence to best practices, without necessarily executing them (acting as a "static analyzer").
-   **`StyleOptimizerAgent`**: Refines prompts to enhance their style, tone, clarity, and persuasiveness, often based on specific target audience or desired communication effect.
-   **`ResultsEvaluatorAgent`**: Assesses the quality, relevance, and effectiveness of the outputs generated by an LLM in response to a given prompt.
-   **`MetaLearnerAgent`**: Analyzes the overall performance of the prompt generation and optimization process over time to provide higher-level guidance and adapt strategies.
-   **`DomainExpertAgent`**: Provides domain-specific knowledge, constraints, terminology, and evaluation criteria to other agents.

### Agent Functionality Notes
The current implementations of these agents are functional placeholders. They utilize mock data (e.g., for templates, critique rules, style rules, domain knowledge) and simplified internal logic. This approach is intended to establish the foundational framework of the multi-agent system and demonstrate the intended interactions and data flows. Future development will involve replacing mock data with configurable sources and implementing more sophisticated algorithms and machine learning models within each agent.

### Message Flow

Agents interact through an asynchronous message bus. Agents subscribe to message
types they care about and broadcast results for others to consume. The
`ResultsEvaluatorAgent` and `PromptCriticAgent` broadcast `evaluation_result` and
`critique_result` messages respectively. The `MetaLearnerAgent` listens for these
messages and updates its knowledge base whenever new feedback arrives.

## Testing

Unit tests for the core placeholder functionalities of each agent and the genetic algorithm components are located in the `prompthelix/tests/unit/` directory. Each component has its own test file (e.g., `test_architect_agent.py`, `test_genetic_operators.py`, etc.) that verifies its basic operations.

To run the tests, navigate to the root directory of the project and use a test runner like Python's `unittest` module. For example:
```bash
python -m unittest discover -s prompthelix/tests/unit
```
(Further details on test execution and integration tests will be added as the project matures.)

## Command-Line Interface (CLI)

The CLI is available via `python -m prompthelix.cli`. It provides several commands for interacting with the PromptHelix system.

### `run` command

The `run` command is used to execute modules within PromptHelix. Currently, its primary use is to run the Genetic Algorithm (GA) for prompt optimization.

**Usage:** `python -m prompthelix.cli run [module_name] [options]`

If `module_name` is omitted, it defaults to `ga`.

**`run ga` Options:**

The following options are available when running the Genetic Algorithm (`python -m prompthelix.cli run ga`):

*   `module`: (Optional) The name of the module to run. Defaults to `ga`.
*   `--prompt "<string>`: (Optional) Provide an initial custom prompt string to seed the first generation of the GA.
*   `--task-description "<string>"`: (Optional) A detailed description of the task the generated prompts should accomplish. This helps guide the GA.
*   `--keywords <word1> <word2> ...`: (Optional) A list of keywords relevant to the task. These can be used by agents to focus prompt generation.
*   `--num-generations <integer>`: (Optional) The number of generations the GA should run for.
*   `--parallel-workers <integer>`: (Optional) Number of parallel workers used for fitness evaluation. Use `1` for serial execution. Default uses available CPU cores.
*   `--population-size <integer>`: (Optional) The number of prompts (chromosomes) in each generation.
*   `--elitism-count <integer>`: (Optional) The number of the best prompts from one generation to carry over directly to the next.
*   `--output-file <filepath>`: (Optional) Specify a file path to save the best prompt found by the GA at the end of the run.
*   `--agent-settings <json_string_or_filepath>`: (Optional) Override default agent configurations. This can be a direct JSON string or a path to a JSON file.
    *   *JSON Format*: The JSON should be a dictionary where keys are agent class names (e.g., "PromptArchitectAgent") and values are dictionaries of settings for that agent. Example: `'{"PromptArchitectAgent": {"default_llm_model": "gpt-4o-mini"}}'`
*   `--llm-settings <json_string_or_filepath>`: (Optional) Override default LLM utility settings (e.g., timeouts, API keys if applicable). This can be a direct JSON string or a path to a JSON file.
    *   *JSON Format*: The JSON should be a dictionary of LLM parameters. Example: `'{"default_timeout": 120, "temperature": 0.8}'`
*   `--execution-mode <TEST|REAL>`: (Optional) Set the execution mode. `TEST` mode uses mock LLM calls and is faster. `REAL` mode makes actual calls to configured LLM providers. Defaults to `TEST`.

**Examples for `run ga`:**

*   Run GA with a specific seed prompt:
    ```bash
    python -m prompthelix.cli run ga --prompt "Generate a short poem about a robot learning to dream."
    ```

*   Run GA with detailed task and GA parameters, saving the best prompt:
    ```bash
    python -m prompthelix.cli run ga --task-description "Create an engaging marketing slogan for a new eco-friendly coffee brand." --keywords "sustainable" "organic" "fresh" --num-generations 20 --population-size 50 --output-file ./marketing_slogan_prompt.txt
    ```

*   Override settings for `PromptArchitectAgent` using a JSON string:
    ```bash
    python -m prompthelix.cli run ga --agent-settings '{"PromptArchitectAgent": {"default_llm_model": "gpt-4o-mini", "knowledge_file_path": "custom_architect_rules.json"}}'
    ```

*   Override general LLM utility settings from a file:
    ```bash
    python -m prompthelix.cli run ga --llm-settings ./configs/custom_llm_params.json
    ```

*   Run GA in `REAL` mode (ensure API keys are set):
    ```bash
    python -m prompthelix.cli run ga --execution-mode REAL --prompt "A real test with an LLM."
    ```
*   Run GA with multiple parallel workers:
    ```bash
    python -m prompthelix.cli run ga --parallel-workers 4
    ```

### LLM Connectivity Test

A utility script `prompthelix/tests/test_llm_connectivity.py` is provided to test connectivity to a specified Large Language Model (LLM) provider and model. This script helps ensure that your environment is correctly configured and that the LLM services are accessible.

**Running the Script:**

You can run the script from the command line using Python:

```bash
python prompthelix/tests/test_llm_connectivity.py [options]
```

**Command-Line Arguments:**

-   `--provider`: Specifies the LLM provider.
    -   Default: `openai`
    -   Example: `openai`, `anthropic`, `google` (adjust based on actual provider keys in `llm_integrations`)
-   `--model`: Specifies the model name for the chosen provider.
    -   Default: `gpt-3.5-turbo`
    -   Example: `gpt-3.5-turbo` (for OpenAI), `claude-2` (for Anthropic), `gemini-pro` (for Google)

**Important Note:**
This script requires the appropriate API key environment variables to be set for the provider you are testing (e.g., `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`). Refer to the "Environment Variable Setup" section in the main `README.md` for details on how to set these.

**Usage Examples:**

```bash
python -m prompthelix.tests.test_llm_connectivity --provider openai --model gpt-3.5-turbo
```

```bash
python -m prompthelix.tests.test_llm_connectivity --provider anthropic --model claude-2
```

You can also run the connectivity check via the CLI:

```bash
python -m prompthelix.cli check-llm --provider openai --model gpt-3.5-turbo
```

Debug logs are printed to the console to aid troubleshooting.

## Genetic Algorithm Engine

PromptHelix employs a Genetic Algorithm (GA) to iteratively evolve and optimize prompts. This engine uses concepts like selection, crossover, and mutation to refine a population of prompts over generations, aiming to enhance their effectiveness based on defined fitness criteria. The GA is designed to interact with various specialized agents for tasks like initial prompt creation, fitness evaluation (based on LLM output simulation), and potentially for more advanced "smart" mutations.

For a detailed explanation of the GA components and flow, please see:
`[Read more about the Genetic Algorithm](./genetic_algorithm.md)`

## Running Parts of the Application

For instructions on running the main Web UI or the CLI for tasks like running the genetic algorithm or tests, please refer to the main `README.md` in the root directory. The main `README.md` covers:

*   Setting up the virtual environment.
*   Installing dependencies.
*   Setting up environment variables (including API keys).
*   Running the FastAPI web server.
*   Using the `prompthelix.cli` for various operations.

The `prompthelix/orchestrator.py` mentioned in previous versions of this document has likely been integrated into or superseded by the CLI and API functionalities described in the main `README.md`.

## Future Directions
(This section can be maintained here if it pertains to more granular, internal development plans not covered in the main README.)

## License

This project is licensed under the MIT License. See the [LICENSE](../../LICENSE) file in the root directory for details.

## Contributing

We welcome contributions! Please see our [CONTRIBUTING.md](../../CONTRIBUTING.md) file in the root directory for detailed guidelines.
