from pydantic import BaseModel, Field
from datetime import datetime
from typing import List, Optional, Dict, Any
from prompthelix.enums import ExecutionMode

# --- User Schemas ---
class UserBase(BaseModel):
    username: str
    email: str

class UserCreate(UserBase):
    password: str

class UserUpdate(BaseModel):
    email: Optional[str] = None
    password: Optional[str] = None

class User(UserBase):
    id: int
    created_at: datetime
    # is_active: bool = True # Omitted for now, as not in the current User model

    class Config:
        from_attributes = True

# --- Session Schemas ---
class SessionBase(BaseModel):
    session_token: str # Token is generated by service, but base can include it for reading
    user_id: int

class SessionCreate(BaseModel): # user_id will be passed to service, token generated there
    # No fields needed here if user_id is a path/body parameter and token is generated by service
    # However, if created via a schema that needs user_id:
    # user_id: int # This would typically be part of the service call, not this schema directly
    pass

class Session(SessionBase):
    id: int
    created_at: datetime
    expires_at: datetime

    class Config:
        from_attributes = True

# Token schema used for authentication responses
class Token(BaseModel):
    access_token: str
    token_type: str

# --- PerformanceMetric Schemas ---
class PerformanceMetricBase(BaseModel):
    metric_name: str
    metric_value: float

class PerformanceMetricCreate(PerformanceMetricBase):
    prompt_version_id: int

class PerformanceMetricUpdate(BaseModel):
    metric_name: Optional[str] = None
    metric_value: Optional[float] = None

class PerformanceMetric(PerformanceMetricBase):
    id: int
    prompt_version_id: int
    created_at: datetime

    class Config:
        from_attributes = True

# --- Prompt Schemas (Updated/Verified) ---
class PromptVersionBase(BaseModel):
    content: str
    parameters_used: Optional[Dict[str, Any]] = None
    fitness_score: Optional[float] = None

class PromptVersionCreate(PromptVersionBase):
    # prompt_id is passed directly to the service method
    pass

class PromptVersionUpdate(BaseModel): # All fields optional for update
    content: Optional[str] = None
    parameters_used: Optional[Dict[str, Any]] = None
    fitness_score: Optional[float] = None

class PromptVersion(PromptVersionBase):
    id: int
    prompt_id: int
    version_number: int
    created_at: datetime

    class Config:
        from_attributes = True

class PromptBase(BaseModel):
    name: str
    description: Optional[str] = None

class PromptCreate(PromptBase):
    pass

class PromptUpdate(PromptBase): # Fields are optional for update
    name: Optional[str] = None
    description: Optional[str] = None
    # Ensure PromptBase itself doesn't make them optional if PromptCreate needs them

class Prompt(PromptBase):
    id: int
    owner_id: int
    created_at: datetime
    versions: List[PromptVersion] = []

    class Config:
        from_attributes = True

# --- APIKey Schemas (Verified/Updated) ---
class APIKeyBase(BaseModel):
    service_name: str = Field(..., description="The name of the LLM service (e.g., OPENAI, ANTHROPIC, GOOGLE)")

class APIKeyCreate(APIKeyBase):
    api_key: str = Field(..., description="The API key for the service.")
    # service_name is inherited

class APIKey(APIKeyBase): # Schema for reading/returning the APIKey model
    id: int
    # api_key: str # Usually not returned directly after creation for security,
                   # but if the model has it and it's needed for internal display/use.
                   # The APIKeyDisplay schema is better for client-facing display.
    class Config:
        from_attributes = True

class APIKeyDisplay(BaseModel): # For displaying API key status safely
    id: int # Added id here for easier reference if needed
    service_name: str
    api_key_hint: Optional[str] = Field(None, description="A non-sensitive hint of the API key (e.g., last 4 characters or 'Set')")
    is_set: bool = Field(..., description="True if the API key is set, False otherwise")

    class Config:
        from_attributes = True


# --- GA Experiment Schemas ---
class GAExperimentParams(BaseModel):
    task_description: str
    execution_mode: ExecutionMode = ExecutionMode.REAL
    keywords: List[str] = Field(default_factory=list)
    num_generations: int = 10
    population_size: int = 20
    elitism_count: int = 2
    parent_prompt_id: Optional[int] = None
    prompt_name: Optional[str] = None # Name for the new prompt created by GA
    prompt_description: Optional[str] = None # Description for the new prompt

# GAExperimentResult can be represented by schemas.PromptVersion.

# --- LLM Testing and Statistics Schemas ---
class LLMTestRequest(BaseModel):
    llm_service: str = Field(..., description="The LLM service to use (e.g., OpenAI, Anthropic, Google)")
    prompt_text: str = Field(..., description="The text prompt to send to the LLM")

class LLMTestResponse(BaseModel):
    llm_service: str = Field(..., description="The LLM service used")
    response_text: str = Field(..., description="The text response from the LLM")

class LLMStatistic(BaseModel): # Represents LLMUsageStatistic model
    llm_service: str = Field(..., description="The LLM service")
    request_count: int = Field(..., description="The number of requests made to this LLM service")
    # last_used_at: Optional[datetime] = None # If you add this to your model

    class Config:
        from_attributes = True

# Schema for creating/updating LLMUsageStatistic
class LLMUsageStatisticCreate(BaseModel):
    llm_service: str
    request_count: int = 1 # Default to 1 for incrementing or initial creation

class LLMUsageStatisticUpdate(BaseModel):
    request_count: int

# --- ConversationLog Schemas ---
class ConversationLogBase(BaseModel):
    timestamp: datetime
    session_id: str
    sender_id: str
    recipient_id: Optional[str] = None
    message_type: Optional[str] = None
    content: str # Should be str, as it's stored as JSON string in DB

class ConversationLogEntry(ConversationLogBase):
    id: int

    class Config:
        from_attributes = True # For SQLAlchemy model conversion (orm_mode in Pydantic v1)

class ConversationSession(BaseModel):
    session_id: str
    message_count: int
    first_message_at: datetime
    last_message_at: datetime

# --- GA Status Schema ---
class GAStatusResponse(BaseModel):
    status: str
    generation: int
    population_size: int
    best_fitness: Optional[float] = None
    fittest_individual_id: Optional[str] = None # Using str for UUID representation
    fittest_chromosome_string: Optional[str] = None
    agents_used: Optional[List[str]] = []
    # Fields from GeneticAlgorithmRunner.get_status()
    runner_current_generation: Optional[int] = None
    runner_target_generations: Optional[int] = None
    runner_population_manager_id: Optional[int] = None
    # Fields from PopulationManager often included in status broadcasts
    is_paused: Optional[bool] = None
    should_stop: Optional[bool] = None
