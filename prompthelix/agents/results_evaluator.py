from prompthelix.agents.base import BaseAgent
from prompthelix.genetics.engine import PromptChromosome
from prompthelix.utils.llm_utils import call_llm_api
from prompthelix.evaluation import metrics as ph_metrics # Added import
import random  # For placeholder metric generation
import json  # For parsing LLM responses
import logging
import os
import asyncio
from prompthelix.config import AGENT_SETTINGS, KNOWLEDGE_DIR # Keep KNOWLEDGE_DIR for default path construction
from typing import Optional, Dict # Added for type hinting

logger = logging.getLogger(__name__)

# Default knowledge filename if nothing else is provided
FALLBACK_KNOWLEDGE_FILE = "results_evaluator_config.json"


class ResultsEvaluatorAgent(BaseAgent):
    agent_id = "ResultsEvaluator"
    agent_description = "Evaluates LLM outputs and assigns fitness scores."
    """
    Assesses the quality, relevance, and effectiveness of the outputs 
    generated by an LLM in response to a given prompt.
    """
    def __init__(self, message_bus=None, settings: Optional[Dict] = None, knowledge_file_path: Optional[str] = None):
        """
        Initializes the ResultsEvaluatorAgent.
        Loads evaluation metric configurations and agent settings.

        Args:
            message_bus (object, optional): The message bus for inter-agent communication.
            settings (Optional[Dict], optional): Configuration settings for the agent.
            knowledge_file_path (Optional[str], optional): Path to the knowledge file.
                Overrides settings['knowledge_file_path'] if provided.
        """
        super().__init__(agent_id="ResultsEvaluator", message_bus=message_bus, settings=settings)

        # Core LLM and fitness settings
        global_defaults = AGENT_SETTINGS.get("ResultsEvaluatorAgent", {})
        llm_provider_default = global_defaults.get("default_llm_provider", "openai")
        eval_model_default = global_defaults.get("evaluation_llm_model", "gpt-3.5-turbo")
        fitness_default = global_defaults.get(
            "fitness_score_weights",
            {"constraint_adherence": 0.5, "llm_quality_assessment": 0.5},
        )

        self.llm_provider = self.settings.get("default_llm_provider", llm_provider_default)
        self.evaluation_llm_model = self.settings.get("evaluation_llm_model", eval_model_default)
        self.fitness_score_weights = self.settings.get("fitness_score_weights", fitness_default)

        # Determine knowledge file path, preferring explicit arg, then settings, then fallback
        _kfp = knowledge_file_path or self.settings.get("knowledge_file_path")
        if _kfp:
            # Absolute paths are used as-is; relative paths are placed under KNOWLEDGE_DIR
            if os.path.isabs(_kfp):
                self.knowledge_file_path = _kfp
            else:
                self.knowledge_file_path = os.path.join(KNOWLEDGE_DIR, _kfp)
        else:
            # Use fallback filename in default directory
            self.knowledge_file_path = os.path.join(KNOWLEDGE_DIR, FALLBACK_KNOWLEDGE_FILE)

        # Ensure the directory exists
        os.makedirs(os.path.dirname(self.knowledge_file_path), exist_ok=True)

        self.logger = logger # Assign module-level logger to instance
        self.logger.info(
            f"Agent '{self.agent_id}' initialized with LLM provider: {self.llm_provider}, "
            f"Evaluation model: {self.evaluation_llm_model}, Knowledge file: {self.knowledge_file_path}"
        )

        # Prepare for loading and storing metric configurations
        self.evaluation_metrics_config = {}
        self.db = None  # call_llm_api will handle DB if needed
        self.load_knowledge() # Loads general evaluator config, not metric functions directly

        # --- New attributes for evaluate_prompt ---
        self.metric_functions = {}
        metric_names_to_load = [
            "calculate_exact_match",
            "calculate_keyword_overlap",
            "calculate_bleu_score", # Placeholder, but compatible signature
            "calculate_output_length" # Ignores second arg, compatible
        ]
        for func_name in metric_names_to_load:
            func_obj = getattr(ph_metrics, func_name, None)
            if func_obj:
                # Use a simplified key, e.g., "exact_match" from "calculate_exact_match"
                metric_key = func_name.replace("calculate_", "")
                self.metric_functions[metric_key] = func_obj
                logger.info(f"Agent '{self.agent_id}': Successfully loaded metric function '{func_name}' as '{metric_key}'.")
            else:
                logger.warning(f"Agent '{self.agent_id}': Failed to load metric function '{func_name}' from ph_metrics module.")

        self.metric_weights = self.settings.get("metric_weights", {})
        logger.info(f"Agent '{self.agent_id}': Metric weights loaded: {self.metric_weights}")
        # --- End of new attributes ---


    def _get_fallback_llm_metrics(self, errors: list = None, status_message: str = 'fallback_due_to_error') -> dict:
        """
        Provides a fallback dictionary of LLM-derived metrics when analysis fails.
        """
        error_message = "; ".join(errors) if errors else "N/A"
        logger.warning(f"Agent '{self.agent_id}': Using fallback LLM metrics. Reason: {error_message}")
        return {
            'llm_assessed_relevance': 0.0,
            'llm_assessed_coherence': 0.0,
            'llm_assessed_completeness': 0.0,
            'llm_accuracy_assessment': 'N/A',
            'llm_safety_score': 0.0,
            'llm_assessed_quality': 0.0,
            'llm_assessment_feedback': f"Fallback: {error_message}",
            'llm_analysis_status': status_message
        }

    def _get_default_metrics_config(self) -> dict:
        """
        Provides default mock configurations for evaluation metrics.

        In a real scenario, this would load from a configuration file,
        a database, or be influenced by the DomainExpertAgent or MetaLearnerAgent.

        Returns:
            dict: A dictionary of metric configurations.
        """
        logger.info(f"Agent '{self.agent_id}': Using default metrics configuration.")
        return {
            "default_metrics": ["relevance_placeholder", "coherence_placeholder"],
            "task_specific": {
                "summarization": ["rouge_score_placeholder", "conciseness_placeholder"],
                "code_generation": ["execution_success_placeholder", "code_quality_placeholder"]
            }
        }

    def load_knowledge(self):
        """
        Loads evaluation metrics configuration from the specified JSON file.
        If the file is not found or is invalid, it loads default config
        and saves it to a new file.
        """
        try:
            with open(self.knowledge_file_path, 'r') as f:
                self.evaluation_metrics_config = json.load(f)
            logger.info(f"Agent '{self.agent_id}': Metrics config loaded successfully from '{self.knowledge_file_path}'.")
        except FileNotFoundError:
            logger.warning(f"Agent '{self.agent_id}': Knowledge file '{self.knowledge_file_path}' not found. Using default config and creating the file.")
            self.evaluation_metrics_config = self._get_default_metrics_config()
            self.save_knowledge() # Save defaults if file not found
        except json.JSONDecodeError as e:
            logger.error(
                f"Agent '{self.agent_id}': Error decoding JSON from '{self.knowledge_file_path}': {e}. Using default config.",
                exc_info=True,
            )
            logging.error(
                f"Agent '{self.agent_id}': Error decoding JSON from '{self.knowledge_file_path}': {e}. Using default config.",
                exc_info=True,
            )
            self.evaluation_metrics_config = self._get_default_metrics_config()
        except Exception as e:
            logger.error(
                f"Agent '{self.agent_id}': Failed to load metrics config from '{self.knowledge_file_path}': {e}. Using default config.",
                exc_info=True,
            )
            logging.error(
                f"Agent '{self.agent_id}': Failed to load metrics config from '{self.knowledge_file_path}': {e}. Using default config.",
                exc_info=True,
            )
            self.evaluation_metrics_config = self._get_default_metrics_config()

    def save_knowledge(self):
        """
        Saves the current evaluation metrics configuration to the specified JSON file.
        """
        try:
            with open(self.knowledge_file_path, 'w') as f:
                json.dump(self.evaluation_metrics_config, f, indent=4)
            logger.info(f"Agent '{self.agent_id}': Metrics config saved successfully to '{self.knowledge_file_path}'.")
        except Exception as e:
            logger.error(f"Agent '{self.agent_id}': Failed to save metrics config to '{self.knowledge_file_path}': {e}", exc_info=True)

    def _calculate_standard_metrics(self, llm_output: str, task_desc: str) -> dict:
        """
        Placeholder method to simulate calculation of standard metrics.

        Args:
            llm_output (str): The output from the LLM.
            task_desc (str): The original task description.

        Returns:
            dict: A dictionary of calculated standard metrics.
        """
        logger.info(f"Agent '{self.agent_id}': Calculating standard metrics for output (len: {len(llm_output)}), task: '{task_desc[:50]}...'")
        # These are very basic metrics, not requiring an LLM.
        # LLM-based metrics will be added/updated in _analyze_content.
        metrics = {
            "output_length": len(llm_output),
            "output_word_count": len(llm_output.split()),
            # Initialize placeholders that LLM will hopefully fill
            "llm_assessed_relevance": 0.0,
            "llm_assessed_coherence": 0.0,
            "llm_assessed_completeness": 0.0,
            "llm_accuracy_assessment": "N/A",
            "llm_safety_score": 0.0,
            "llm_assessed_quality": 0.0,
            "llm_assessment_feedback": "N/A"
        }
        # Add legacy random placeholders for comparison or if LLM fails badly
        metrics["relevance_placeholder"] = round(random.uniform(0.5, 0.95), 2) # Kept for potential direct use or comparison
        metrics["coherence_placeholder"] = round(random.uniform(0.4, 0.9), 2) # Kept for potential direct use or comparison

        logger.info(f"Agent '{self.agent_id}': Basic non-LLM metrics calculated: {metrics}")
        return metrics

    def _check_constraints(self, llm_output: str, success_criteria: dict) -> dict:
        """
        Checks if the LLM output meets constraints defined in success_criteria.

        Args:
            llm_output (str): The output from the LLM.
            success_criteria (dict): A dictionary defining success criteria.
                                     Example: {"max_length": 100, "must_include_keywords": ["AI", "ethics"]}

        Returns:
            dict: Contains "metrics" (e.g., {"constraint_adherence": 0.0/1.0}) 
                  and "errors" (list of strings for violations).
        """
        errors = []
        metrics = {}
        constraints_met = 0
        total_constraints = 0

        if not success_criteria:
            metrics["constraint_adherence_placeholder"] = 1.0 # No constraints to violate; initialize this key
            return {"metrics": metrics, "errors": errors}

        logger.info(f"Agent '{self.agent_id}': Checking constraints: {success_criteria}")

        # Length check
        if "max_length" in success_criteria:
            total_constraints += 1
            if len(llm_output) > success_criteria["max_length"]:
                errors.append(f"Constraint Violation: Output length ({len(llm_output)}) exceeds max_length ({success_criteria['max_length']}).")
            else:
                constraints_met += 1
        
        if "min_length" in success_criteria:
            total_constraints +=1
            if len(llm_output) < success_criteria["min_length"]:
                errors.append(f"Constraint Violation: Output length ({len(llm_output)}) is less than min_length ({success_criteria['min_length']}).")
            else:
                constraints_met +=1

        # Keyword check
        if "must_include_keywords" in success_criteria:
            missing_keywords = []
            for kw in success_criteria["must_include_keywords"]:
                total_constraints += 1 # Each keyword is a constraint
                if kw.lower() not in llm_output.lower():
                    missing_keywords.append(kw)
                else:
                    constraints_met += 1
            if missing_keywords:
                errors.append(f"Constraint Violation: Output is missing required keywords: {', '.join(missing_keywords)}.")
        
        if "must_not_include_keywords" in success_criteria:
            present_forbidden_keywords = []
            for kw in success_criteria["must_not_include_keywords"]:
                total_constraints += 1
                if kw.lower() in llm_output.lower():
                    present_forbidden_keywords.append(kw)
                else:
                    constraints_met +=1 # Not finding a forbidden keyword means constraint is met
            if present_forbidden_keywords:
                errors.append(f"Constraint Violation: Output includes forbidden keywords: {', '.join(present_forbidden_keywords)}.")


        if total_constraints > 0:
            metrics["constraint_adherence_placeholder"] = round(constraints_met / total_constraints, 2)
        else:
            metrics["constraint_adherence_placeholder"] = 1.0 # No applicable constraints; ensure key is present

        logger.info(f"Agent '{self.agent_id}': Constraint check result - Metrics={metrics}, Errors#={len(errors)}")
        return {"metrics": metrics, "errors": errors}

    def _analyze_content(self, llm_output: str, task_desc: str, prompt_chromosome: PromptChromosome) -> tuple[dict, list]:
        """
        Analyzes LLM output content using another LLM for quality, relevance, coherence, etc.
        Falls back to placeholder values if LLM analysis fails.

        Args:
            llm_output (str): The output from the LLM to be evaluated.
            task_desc (str): The original task description.
            prompt_chromosome (PromptChromosome): The prompt that generated the output.

        Returns:
            tuple[dict, list]: A tuple containing a dictionary of LLM-assessed content metrics
                               and a list of error/warning strings from the analysis.
        """
        logger.info(f"Agent '{self.agent_id}': LLM Analyzing content for task: '{task_desc[:50]}...' using model {self.evaluation_llm_model}")

        # Defined error strings from llm_utils.py
        LLM_API_ERROR_STRINGS = {
            "RATE_LIMIT_ERROR", "API_KEY_MISSING_ERROR", "AUTHENTICATION_ERROR",
            "API_CONNECTION_ERROR", "INVALID_REQUEST_ERROR", "API_ERROR", "OPENAI_ERROR",
            "UNEXPECTED_OPENAI_CALL_ERROR", "ANTHROPIC_ERROR", "UNEXPECTED_ANTHROPIC_CALL_ERROR",
            "GOOGLE_SDK_ERROR", "UNEXPECTED_GOOGLE_CALL_ERROR", "BLOCKED_PROMPT_ERROR",
            "API_STATUS_ERROR", "API_SERVER_ERROR", "EMPTY_GOOGLE_RESPONSE", "INVALID_ARGUMENT_ERROR", # Added INVALID_ARGUMENT_ERROR for Google
            "MALFORMED_CLAUDE_RESPONSE_CONTENT", "EMPTY_CLAUDE_RESPONSE", "UNSUPPORTED_PROVIDER_ERROR",
            "GENERATION_STOPPED_SAFETY", "GENERATION_STOPPED_RECITATION" # Common Google stop reasons
            # Any other specific error strings returned by llm_utils.py should be added here.
        }

        prompt_str_for_llm = f"""
You are an expert evaluator of AI-generated text. Evaluate the "Generated Output" based on the "Original Task Description" and the "Prompt Used".

Original Task Description:
{task_desc}

Prompt Used:
{str(prompt_chromosome)}

Generated Output:
{llm_output}

---
Evaluation Criteria:
1.  **Relevance**: How relevant is the output to the task description and prompt? (Score 0.0-1.0)
2.  **Coherence**: Is the output well-structured, logical, and easy to understand? (Score 0.0-1.0)
3.  **Completeness**: Does the output address all key aspects of the task? (Score 0.0-1.0)
4.  **Accuracy**: (Qualitative, if possible) Does the output seem accurate and avoid misinformation? If you cannot assess accuracy, state "N/A".
5.  **Safety/Harmlessness**: Does the output avoid harmful, biased, or inappropriate content? (Boolean: True/False, or Score 0.0-1.0 where 1.0 is safe)
6.  **Overall Quality**: What is the overall quality of the output? (Score 0.0-1.0)
7.  **Brief Feedback**: Provide a short (1-2 sentence) justification for your scores, highlighting strengths or weaknesses.

Return your evaluation as a JSON object with keys: "relevance_score", "coherence_score", "completeness_score", "accuracy_assessment" (string), "safety_score", "overall_quality_score", "feedback_text".
Example:
{{
  "relevance_score": 0.8,
  "coherence_score": 0.9,
  "completeness_score": 0.7,
  "accuracy_assessment": "Appears factually sound based on provided context.",
  "safety_score": 1.0,
  "overall_quality_score": 0.85,
  "feedback_text": "The output is highly relevant and coherent, but could be more complete in addressing X."
}}
"""
        llm_derived_metrics = {}
        errors = [] # Errors specific to this analysis process

        # Pass self.db if available and needed by call_llm_api. Assuming self.db might be None.
        # call_llm_api is designed to handle db=None.
        response_str = call_llm_api(prompt_str_for_llm, provider=self.llm_provider, model=self.evaluation_llm_model, db=self.db)

        if response_str in LLM_API_ERROR_STRINGS:
            logger.warning(f"Agent '{self.agent_id}': LLM call for content analysis failed with error code: {response_str}. Using fallback metrics.")
            api_error_message = f"LLM API Error: {response_str}"
            errors.append(api_error_message)
            llm_derived_metrics = self._get_fallback_llm_metrics(errors=[api_error_message])
        else:
            # LLM call was successful (not an error string), now try to parse its response
            logger.info(f"Agent '{self.agent_id}': LLM raw response for content analysis: {response_str[:500]}...") # Log snippet
            try:
                # Attempt to parse JSON from the LLM response
                json_start = response_str.find('{')
                json_end = response_str.rfind('}') + 1
                if json_start != -1 and json_end != -1 and json_start < json_end:
                    parsed_response = json.loads(response_str[json_start:json_end])

                    llm_derived_metrics["llm_assessed_relevance"] = float(parsed_response.get("relevance_score", 0.0))
                    llm_derived_metrics["llm_assessed_coherence"] = float(parsed_response.get("coherence_score", 0.0))
                    llm_derived_metrics["llm_assessed_completeness"] = float(parsed_response.get("completeness_score", 0.0))
                    llm_derived_metrics["llm_accuracy_assessment"] = str(parsed_response.get("accuracy_assessment", "N/A"))
                    llm_derived_metrics["llm_safety_score"] = float(parsed_response.get("safety_score", 0.0))
                    llm_derived_metrics["llm_assessed_quality"] = float(parsed_response.get("overall_quality_score", 0.0))
                    llm_derived_metrics["llm_assessment_feedback"] = str(parsed_response.get("feedback_text", "No textual feedback from LLM."))
                    llm_derived_metrics['llm_analysis_status'] = 'success'
                    logger.info(f"Agent '{self.agent_id}': LLM content analysis parsed successfully.")
                else:
                    parsing_error_message = "LLM content analysis response was not in expected JSON format."
                    errors.append(parsing_error_message)
                    logger.warning(f"Agent '{self.agent_id}': {parsing_error_message} Raw response snippet: {response_str[:200]}. Using fallback.")
                    llm_derived_metrics = self._get_fallback_llm_metrics(errors=[parsing_error_message, f"Raw Response Snippet: {response_str[:200]}"])

            except json.JSONDecodeError as je:
                json_error_message = f"JSON decoding failed: {str(je)}"
                errors.append(json_error_message)
                logger.error(f"Agent '{self.agent_id}': Error parsing LLM evaluation response (JSONDecodeError): {je}. Raw response snippet: {response_str[:200]}", exc_info=True)
                llm_derived_metrics = self._get_fallback_llm_metrics(errors=[json_error_message, f"Raw Response Snippet: {response_str[:200]}"])
            except Exception as e: # Catch other potential errors during parsing (e.g., float conversion)
                parsing_exception_message = f"Parsing LLM response failed: {str(e)}"
                errors.append(parsing_exception_message)
                logger.error(f"Agent '{self.agent_id}': Error parsing LLM evaluation response: {e}. Raw response snippet: {response_str[:200]}", exc_info=True)
                llm_derived_metrics = self._get_fallback_llm_metrics(errors=[parsing_exception_message, f"Raw Response Snippet: {response_str[:200]}"])

            # Final check if metrics were populated, even if parsing seemed to start
            if not llm_derived_metrics or llm_derived_metrics.get('llm_analysis_status') != 'success':
                 # This case handles if parsed_response was empty or some other logic path was missed.
                if not llm_derived_metrics.get('llm_analysis_status'): # Avoid overwriting if already fallback
                    logger.warning(f"Agent '{self.agent_id}': LLM evaluation response parse attempt did not yield metrics or status. Raw response snippet: {response_str[:200]}. Using fallback.")
                    unparseable_error = "LLM response unparseable or empty after attempt."
                    if not errors: errors.append(unparseable_error) # Add if no specific error was caught yet
                    llm_derived_metrics = self._get_fallback_llm_metrics(errors=errors if errors else [unparseable_error])


        # Ensure all expected keys are present, even if using fallback (which _get_fallback_llm_metrics should do)
        # This is more of a safeguard. _get_fallback_llm_metrics should be comprehensive.
        expected_metric_keys = [
            'llm_assessed_relevance', 'llm_assessed_coherence', 'llm_assessed_completeness',
            'llm_accuracy_assessment', 'llm_safety_score', 'llm_assessed_quality',
            'llm_assessment_feedback', 'llm_analysis_status'
        ]
        for key in expected_metric_keys:
            if key not in llm_derived_metrics:
                logger.error(f"CRITICAL: Fallback metrics are missing key '{key}'. This should not happen. Re-applying generic fallback.")
                llm_derived_metrics = self._get_fallback_llm_metrics(errors=["Critical: Fallback was incomplete. Overridden."])
                if not errors: errors.append("Critical: Fallback logic error, metrics incomplete.")
                break # Exit loop after applying generic fallback

        return llm_derived_metrics, errors

    def process_request(self, request_data: dict) -> dict:
        """
        Evaluates the LLM output for a given prompt, primarily using another LLM for content analysis.

        Args:
            request_data (dict): Expected keys: 
                                 'prompt_chromosome' (PromptChromosome instance), 
                                 'llm_output' (str: the output from the LLM),
                                 'task_description' (str), 
                                 'success_criteria' (dict, optional).
        Returns:
            dict: Contains 'fitness_score' (float), 'detailed_metrics' (dict), 
                  and 'error_analysis' (list of strings).
        """
        prompt_chromosome = request_data.get("prompt_chromosome")
        llm_output = request_data.get("llm_output", "") # This is the output to evaluate
        task_desc = request_data.get("task_description", "")
        success_criteria = request_data.get("success_criteria", {})

        if not isinstance(prompt_chromosome, PromptChromosome):
            logger.error(f"Agent '{self.agent_id}': Invalid prompt_chromosome object received.")
            return {"fitness_score": 0.0, "detailed_metrics": {}, "error_analysis": ["Error: Invalid prompt_chromosome."]}

        # Check for upstream LLM API errors that generated the output being evaluated
        if llm_output.startswith("Error: LLM API call failed.") or \
           llm_output.startswith("Error: No content from LLM."):
            logger.warning(f"Agent '{self.agent_id}': Input LLM output indicates an API error: {llm_output}")
            return {
                "fitness_score": 0.0,
                "detailed_metrics": {"llm_call_status_upstream": "failed", "error_message_upstream": llm_output, "constraint_adherence_placeholder": 0.0},
                "error_analysis": [f"Input LLM generation failed: {llm_output}"]
            }

        logger.info(f"Agent '{self.agent_id}': Evaluating output for prompt: {str(prompt_chromosome)[:100]}... \nLLM Output to Evaluate: {llm_output[:100]}...")

        metrics = {}
        errors = [] # Errors from the evaluation process itself

        metrics.update(self._calculate_standard_metrics(llm_output, task_desc))
        
        constraint_feedback = self._check_constraints(llm_output, success_criteria)
        errors.extend(constraint_feedback.get("errors", [])) # Constraint violations are added to 'errors'
        metrics.update(constraint_feedback.get("metrics", {}))
        
        content_metrics, content_errors = self._analyze_content(llm_output, task_desc, prompt_chromosome)
        metrics.update(content_metrics)
        errors.extend(content_errors) # Errors from the LLM analysis process itself
        
        # Calculate overall fitness score
        constraint_adherence_score = metrics.get("constraint_adherence_placeholder", 0.0)
        llm_quality_assessment = metrics.get("llm_assessed_quality", 0.0)

        # Use weights from config
        weight_constraint = self.fitness_score_weights.get("constraint_adherence", 0.5)
        weight_quality = self.fitness_score_weights.get("llm_quality_assessment", 0.5)

        # If LLM evaluation itself failed, we might rely more on constraint adherence or basic placeholders
        if "LLM analysis not performed or failed" in metrics.get("llm_assessment_feedback", "") or \
           "Exception during LLM analysis" in metrics.get("llm_assessment_feedback", ""):
            logger.warning(f"Agent '{self.agent_id}': LLM quality assessment failed or was unreliable. Adjusting fitness calculation if necessary or relying on placeholders via llm_quality_assessment.")
            # llm_quality_assessment would be a random score from _analyze_content's fallback in this case.
            # Consider if weights should change or if this random score is acceptable.
            # For now, we use the (potentially random) llm_quality_assessment.

        fitness_score = (constraint_adherence_score * weight_constraint) + \
                        (llm_quality_assessment * weight_quality)
        
        # Further penalize if the LLM-based evaluation itself had errors (e.g., malformed JSON from eval LLM)
        # These 'content_errors' are about the evaluation process, not the content being evaluated.
        if content_errors:
            fitness_score -= len(content_errors) * 0.05 # Small penalty for each eval process issue
        
        fitness_score = max(0.0, min(1.0, fitness_score))

        logger.info(f"Agent '{self.agent_id}': Evaluation complete. Fitness: {fitness_score:.4f}, Metrics: {metrics}, Evaluation Process Errors: {len(content_errors)}, Constraint Violations: {len(constraint_feedback.get('errors',[]))}")
        result = {
            "fitness_score": fitness_score,
            "detailed_metrics": metrics,
            "error_analysis": errors,
            "prompt_chromosome": prompt_chromosome,
        }

        # Broadcast the evaluation result if a bus is available
        if self.message_bus:
            try:
                loop = asyncio.get_running_loop()
                loop.create_task(self.message_bus.broadcast_message("evaluation_result", result, sender_id=self.agent_id))
            except RuntimeError:
                asyncio.run(self.message_bus.broadcast_message("evaluation_result", result, sender_id=self.agent_id))

        # Debug logging before returning
        logger.info(f"REA.process_request: Type of prompt_chromosome input: {type(request_data.get('prompt_chromosome'))}")
        logger.info(f"REA.process_request: Returning result with keys: {result.keys()}")
        if 'detailed_metrics' in result:
            logger.info(f"REA.process_request: detailed_metrics keys: {result['detailed_metrics'].keys()}")
            logger.info(f"REA.process_request: llm_analysis_status in detailed_metrics: {result['detailed_metrics'].get('llm_analysis_status')}")
        else:
            logger.warning("REA.process_request: 'detailed_metrics' key is MISSING in the result.")

        return result

    def evaluate_prompt(self, prompt: str, output: str) -> dict:
        """
        Evaluates a given prompt's output using a set of predefined programmatic metrics.

        Args:
            prompt (str): The prompt string (used as reference/expected output for some metrics).
            output (str): The LLM output string to evaluate.

        Returns:
            dict: A dictionary containing the overall 'score' and 'details' of each metric.
        """
        results = {}
        if not self.metric_functions:
            logger.warning(f"Agent '{self.agent_id}': No metric functions loaded for evaluate_prompt. Returning zero score.")
            return {"score": 0.0, "details": {"error": "No metric functions loaded."}}

        logger.info(f"Agent '{self.agent_id}': Evaluating prompt output. Output: '{output[:100]}...', Prompt: '{prompt[:100]}...'")

        for metric_name, metric_func in self.metric_functions.items():
            try:
                # Call metric_func(generated_output, expected_output_or_context)
                # Here, 'output' is generated_output, 'prompt' is expected_output_or_context
                metric_value = metric_func(output, prompt)
                results[metric_name] = metric_value
                logger.debug(f"Agent '{self.agent_id}': Metric '{metric_name}' result: {metric_value}")
            except Exception as e:
                logger.error(f"Agent '{self.agent_id}': Error calculating metric '{metric_name}': {e}", exc_info=True)
                results[metric_name] = 0.0 # Default to 0 on error for that metric

        total_score = 0.0
        total_weight = 0.0

        if not self.metric_weights:
            logger.warning(f"Agent '{self.agent_id}': No metric weights defined. Individual metric scores calculated but overall score will be 0 or based on default weight of 0.")

        for metric_name, metric_value in results.items():
            weight = self.metric_weights.get(metric_name, 0.0) # Default weight 0 if not specified
            if weight < 0: # Ensure weights are not negative
                logger.warning(f"Agent '{self.agent_id}': Negative weight {weight} for metric '{metric_name}' encountered. Using 0 instead.")
                weight = 0.0

            total_score += metric_value * weight
            total_weight += weight
            logger.debug(f"Agent '{self.agent_id}': Metric '{metric_name}' value: {metric_value}, weight: {weight}, current total_score: {total_score}, current total_weight: {total_weight}")


        if total_weight == 0:
            logger.warning(f"Agent '{self.agent_id}': Total weight for metrics is zero. Final score will be 0.0. This may be due to missing or all-zero weights.")
            final_score = 0.0
        else:
            final_score = total_score / total_weight

        logger.info(f"Agent '{self.agent_id}': Prompt evaluation complete. Final Score: {final_score:.4f}, Details: {results}")
        return {"score": round(final_score, 4), "details": results}
