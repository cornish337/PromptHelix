from prompthelix.agents.base import BaseAgent
from prompthelix.genetics.engine import PromptChromosome
from prompthelix.utils.llm_utils import call_llm_api
import random # For placeholder metric generation
import json # For parsing LLM responses
import logging
from prompthelix.config import AGENT_SETTINGS # Import AGENT_SETTINGS

logger = logging.getLogger(__name__)

# Default provider from config if specific agent setting is not found
FALLBACK_LLM_PROVIDER = AGENT_SETTINGS.get("ResultsEvaluatorAgent", {}).get("default_llm_provider", "openai")
FALLBACK_EVAL_MODEL = AGENT_SETTINGS.get("ResultsEvaluatorAgent", {}).get("evaluation_llm_model", "gpt-3.5-turbo") # Default to a basic model if gpt-4 isn't specified/available
DEFAULT_FITNESS_WEIGHTS = {"constraint_adherence": 0.5, "llm_quality_assessment": 0.5}


class ResultsEvaluatorAgent(BaseAgent):
    """
    Assesses the quality, relevance, and effectiveness of the outputs 
    generated by an LLM in response to a given prompt.
    """
    def __init__(self, message_bus=None):
        """
        Initializes the ResultsEvaluatorAgent.
        Loads evaluation metric configurations and agent settings.

        Args:
            message_bus (object, optional): The message bus for inter-agent communication.
        """
        super().__init__(agent_id="ResultsEvaluator", message_bus=message_bus)

        agent_config = AGENT_SETTINGS.get(self.agent_id, {})
        self.llm_provider = agent_config.get("default_llm_provider", FALLBACK_LLM_PROVIDER)
        self.evaluation_llm_model = agent_config.get("evaluation_llm_model", FALLBACK_EVAL_MODEL)
        self.fitness_score_weights = agent_config.get("fitness_score_weights", DEFAULT_FITNESS_WEIGHTS)
        logger.info(f"Agent '{self.agent_id}' initialized with LLM provider: {self.llm_provider}, Evaluation model: {self.evaluation_llm_model}")

        self.knowledge_file_path = knowledge_file_path
        if self.knowledge_file_path is None:
            self.knowledge_file_path = "results_evaluator_config.json"

        self.evaluation_metrics_config = {} # Initialize before loading
        self.load_knowledge()

    def _get_default_metrics_config(self) -> dict:
        """
        Provides default mock configurations for evaluation metrics.

        In a real scenario, this would load from a configuration file,
        a database, or be influenced by the DomainExpertAgent or MetaLearnerAgent.

        Returns:
            dict: A dictionary of metric configurations.
        """
        logger.info(f"Agent '{self.agent_id}': Using default metrics configuration.")
        return {
            "default_metrics": ["relevance_placeholder", "coherence_placeholder"],
            "task_specific": {
                "summarization": ["rouge_score_placeholder", "conciseness_placeholder"],
                "code_generation": ["execution_success_placeholder", "code_quality_placeholder"]
            }
        }

    def load_knowledge(self):
        """
        Loads evaluation metrics configuration from the specified JSON file.
        If the file is not found or is invalid, it loads default config
        and saves it to a new file.
        """
        try:
            with open(self.knowledge_file_path, 'r') as f:
                self.evaluation_metrics_config = json.load(f)
            logger.info(f"Agent '{self.agent_id}': Metrics config loaded successfully from '{self.knowledge_file_path}'.")
        except FileNotFoundError:
            logger.warning(f"Agent '{self.agent_id}': Knowledge file '{self.knowledge_file_path}' not found. Using default config and creating the file.")
            self.evaluation_metrics_config = self._get_default_metrics_config()
            self.save_knowledge() # Save defaults if file not found
        except json.JSONDecodeError as e:
            logger.error(f"Agent '{self.agent_id}': Error decoding JSON from '{self.knowledge_file_path}': {e}. Using default config.", exc_info=True)
            self.evaluation_metrics_config = self._get_default_metrics_config()
        except Exception as e:
            logger.error(f"Agent '{self.agent_id}': Failed to load metrics config from '{self.knowledge_file_path}': {e}. Using default config.", exc_info=True)
            self.evaluation_metrics_config = self._get_default_metrics_config()

    def save_knowledge(self):
        """
        Saves the current evaluation metrics configuration to the specified JSON file.
        """
        try:
            with open(self.knowledge_file_path, 'w') as f:
                json.dump(self.evaluation_metrics_config, f, indent=4)
            logger.info(f"Agent '{self.agent_id}': Metrics config saved successfully to '{self.knowledge_file_path}'.")
        except Exception as e:
            logger.error(f"Agent '{self.agent_id}': Failed to save metrics config to '{self.knowledge_file_path}': {e}", exc_info=True)

    def _calculate_standard_metrics(self, llm_output: str, task_desc: str) -> dict:
        """
        Placeholder method to simulate calculation of standard metrics.

        Args:
            llm_output (str): The output from the LLM.
            task_desc (str): The original task description.

        Returns:
            dict: A dictionary of calculated standard metrics.
        """
        logger.info(f"Agent '{self.agent_id}': Calculating standard metrics for output (len: {len(llm_output)}), task: '{task_desc[:50]}...'")
        # These are very basic metrics, not requiring an LLM.
        # LLM-based metrics will be added/updated in _analyze_content.
        metrics = {
            "output_length": len(llm_output),
            "output_word_count": len(llm_output.split()),
            # Initialize placeholders that LLM will hopefully fill
            "llm_assessed_relevance": 0.0,
            "llm_assessed_coherence": 0.0,
            "llm_assessed_completeness": 0.0,
            "llm_accuracy_assessment": "N/A",
            "llm_safety_score": 0.0,
            "llm_assessed_quality": 0.0,
            "llm_assessment_feedback": "N/A"
        }
        # Add legacy random placeholders for comparison or if LLM fails badly
        metrics["relevance_placeholder"] = round(random.uniform(0.5, 0.95), 2) # Kept for potential direct use or comparison
        metrics["coherence_placeholder"] = round(random.uniform(0.4, 0.9), 2) # Kept for potential direct use or comparison

        logger.info(f"Agent '{self.agent_id}': Basic non-LLM metrics calculated: {metrics}")
        return metrics

    def _check_constraints(self, llm_output: str, success_criteria: dict) -> dict:
        """
        Checks if the LLM output meets constraints defined in success_criteria.

        Args:
            llm_output (str): The output from the LLM.
            success_criteria (dict): A dictionary defining success criteria.
                                     Example: {"max_length": 100, "must_include_keywords": ["AI", "ethics"]}

        Returns:
            dict: Contains "metrics" (e.g., {"constraint_adherence": 0.0/1.0}) 
                  and "errors" (list of strings for violations).
        """
        errors = []
        metrics = {}
        constraints_met = 0
        total_constraints = 0

        if not success_criteria:
            metrics["constraint_adherence_placeholder"] = 1.0 # No constraints to violate; initialize this key
            return {"metrics": metrics, "errors": errors}

        logger.info(f"Agent '{self.agent_id}': Checking constraints: {success_criteria}")

        # Length check
        if "max_length" in success_criteria:
            total_constraints += 1
            if len(llm_output) > success_criteria["max_length"]:
                errors.append(f"Constraint Violation: Output length ({len(llm_output)}) exceeds max_length ({success_criteria['max_length']}).")
            else:
                constraints_met += 1
        
        if "min_length" in success_criteria:
            total_constraints +=1
            if len(llm_output) < success_criteria["min_length"]:
                errors.append(f"Constraint Violation: Output length ({len(llm_output)}) is less than min_length ({success_criteria['min_length']}).")
            else:
                constraints_met +=1

        # Keyword check
        if "must_include_keywords" in success_criteria:
            missing_keywords = []
            for kw in success_criteria["must_include_keywords"]:
                total_constraints += 1 # Each keyword is a constraint
                if kw.lower() not in llm_output.lower():
                    missing_keywords.append(kw)
                else:
                    constraints_met += 1
            if missing_keywords:
                errors.append(f"Constraint Violation: Output is missing required keywords: {', '.join(missing_keywords)}.")
        
        if "must_not_include_keywords" in success_criteria:
            present_forbidden_keywords = []
            for kw in success_criteria["must_not_include_keywords"]:
                total_constraints += 1
                if kw.lower() in llm_output.lower():
                    present_forbidden_keywords.append(kw)
                else:
                    constraints_met +=1 # Not finding a forbidden keyword means constraint is met
            if present_forbidden_keywords:
                errors.append(f"Constraint Violation: Output includes forbidden keywords: {', '.join(present_forbidden_keywords)}.")


        if total_constraints > 0:
            metrics["constraint_adherence_placeholder"] = round(constraints_met / total_constraints, 2)
        else:
            metrics["constraint_adherence_placeholder"] = 1.0 # No applicable constraints; ensure key is present

        logger.info(f"Agent '{self.agent_id}': Constraint check result - Metrics={metrics}, Errors#={len(errors)}")
        return {"metrics": metrics, "errors": errors}

    def _analyze_content(self, llm_output: str, task_desc: str, prompt_chromosome: PromptChromosome) -> tuple[dict, list]:
        """
        Analyzes LLM output content using another LLM for quality, relevance, coherence, etc.
        Falls back to placeholder values if LLM analysis fails.

        Args:
            llm_output (str): The output from the LLM to be evaluated.
            task_desc (str): The original task description.
            prompt_chromosome (PromptChromosome): The prompt that generated the output.

        Returns:
            tuple[dict, list]: A tuple containing a dictionary of LLM-assessed content metrics
                               and a list of error/warning strings from the analysis.
        """
        logger.info(f"Agent '{self.agent_id}': LLM Analyzing content for task: '{task_desc[:50]}...' using model {self.evaluation_llm_model}")

        prompt_str_for_llm = f"""
You are an expert evaluator of AI-generated text. Evaluate the "Generated Output" based on the "Original Task Description" and the "Prompt Used".

Original Task Description:
{task_desc}

Prompt Used:
{str(prompt_chromosome)}

Generated Output:
{llm_output}

---
Evaluation Criteria:
1.  **Relevance**: How relevant is the output to the task description and prompt? (Score 0.0-1.0)
2.  **Coherence**: Is the output well-structured, logical, and easy to understand? (Score 0.0-1.0)
3.  **Completeness**: Does the output address all key aspects of the task? (Score 0.0-1.0)
4.  **Accuracy**: (Qualitative, if possible) Does the output seem accurate and avoid misinformation? If you cannot assess accuracy, state "N/A".
5.  **Safety/Harmlessness**: Does the output avoid harmful, biased, or inappropriate content? (Boolean: True/False, or Score 0.0-1.0 where 1.0 is safe)
6.  **Overall Quality**: What is the overall quality of the output? (Score 0.0-1.0)
7.  **Brief Feedback**: Provide a short (1-2 sentence) justification for your scores, highlighting strengths or weaknesses.

Return your evaluation as a JSON object with keys: "relevance_score", "coherence_score", "completeness_score", "accuracy_assessment" (string), "safety_score", "overall_quality_score", "feedback_text".
Example:
{{
  "relevance_score": 0.8,
  "coherence_score": 0.9,
  "completeness_score": 0.7,
  "accuracy_assessment": "Appears factually sound based on provided context.",
  "safety_score": 1.0,
  "overall_quality_score": 0.85,
  "feedback_text": "The output is highly relevant and coherent, but could be more complete in addressing X."
}}
"""
        # Initialize metrics that should be provided by the LLM to avoid KeyErrors later if LLM fails
        metrics = {
            "llm_assessed_relevance": 0.0,
            "llm_assessed_coherence": 0.0,
            "llm_assessed_completeness": 0.0,
            "llm_accuracy_assessment": "N/A",
            "llm_safety_score": 0.0,
            "llm_assessed_quality": 0.0,
            "llm_assessment_feedback": "LLM analysis not performed or failed."
        }
        errors = []

        try:
            response_str = call_llm_api(prompt_str_for_llm, provider=self.llm_provider, model=self.evaluation_llm_model)
            logger.info(f"Agent '{self.agent_id}': LLM raw response for content analysis: {response_str}")

            # Attempt to parse JSON from the LLM response
            json_start = response_str.find('{')
            json_end = response_str.rfind('}') + 1
            if json_start != -1 and json_end != -1 and json_start < json_end:
                parsed_response = json.loads(response_str[json_start:json_end])

                metrics["llm_assessed_relevance"] = float(parsed_response.get("relevance_score", 0.0))
                metrics["llm_assessed_coherence"] = float(parsed_response.get("coherence_score", 0.0))
                metrics["llm_assessed_completeness"] = float(parsed_response.get("completeness_score", 0.0))
                metrics["llm_accuracy_assessment"] = str(parsed_response.get("accuracy_assessment", "N/A"))
                metrics["llm_safety_score"] = float(parsed_response.get("safety_score", 0.0)) # Assuming 0.0 if unsafe, 1.0 if safe
                metrics["llm_assessed_quality"] = float(parsed_response.get("overall_quality_score", 0.0))
                metrics["llm_assessment_feedback"] = str(parsed_response.get("feedback_text", "No textual feedback from LLM."))
                logger.info(f"Agent '{self.agent_id}': LLM content analysis successful: {metrics}")
            else:
                errors.append("LLM content analysis response was not in expected JSON format.")
                metrics["llm_assessment_feedback"] = f"LLM analysis failed to produce valid JSON. Raw: {response_str[:200]}..." # Already initialized
                logger.error(f"Agent '{self.agent_id}': LLM analysis response not in JSON format. Raw: {response_str}")

        except Exception as e:
            errors.append(f"Error during LLM content analysis: {str(e)}")
            metrics["llm_assessment_feedback"] = f"Exception during LLM analysis: {str(e)}" # Already initialized
            logger.error(f"Agent '{self.agent_id}': Exception during LLM content analysis: {e}", exc_info=True)

        # Fallback for individual scores if not provided by LLM, even if JSON was valid but incomplete
        # The initial metrics dict already provides these defaults, so setdefault isn't strictly needed
        # unless the LLM could return partial valid JSON without some keys.
        if metrics.get("llm_assessed_quality", 0.0) == 0.0 and "LLM analysis not performed or failed" in metrics.get("llm_assessment_feedback"):
            # This implies a more significant failure of the LLM analysis to return any meaningful score.
            metrics["llm_assessed_quality"] = round(random.uniform(0.2, 0.5), 2) # Penalize more if LLM eval failed
            metrics["llm_assessed_relevance"] = round(random.uniform(0.2, 0.5), 2)
            metrics["llm_assessed_coherence"] = round(random.uniform(0.2, 0.5), 2)
            if not errors: errors.append("LLM content analysis did not return usable scores or failed; used random fallbacks.")
            logger.warning(f"Agent '{self.agent_id}': LLM content analysis failed or returned no scores. Using random fallbacks for quality metrics.")

        return metrics, errors

    def process_request(self, request_data: dict) -> dict:
        """
        Evaluates the LLM output for a given prompt, primarily using another LLM for content analysis.

        Args:
            request_data (dict): Expected keys: 
                                 'prompt_chromosome' (PromptChromosome instance), 
                                 'llm_output' (str: the output from the LLM),
                                 'task_description' (str), 
                                 'success_criteria' (dict, optional).
        Returns:
            dict: Contains 'fitness_score' (float), 'detailed_metrics' (dict), 
                  and 'error_analysis' (list of strings).
        """
        prompt_chromosome = request_data.get("prompt_chromosome")
        llm_output = request_data.get("llm_output", "") # This is the output to evaluate
        task_desc = request_data.get("task_description", "")
        success_criteria = request_data.get("success_criteria", {})

        if not isinstance(prompt_chromosome, PromptChromosome):
            logger.error(f"Agent '{self.agent_id}': Invalid prompt_chromosome object received.")
            return {"fitness_score": 0.0, "detailed_metrics": {}, "error_analysis": ["Error: Invalid prompt_chromosome."]}

        # Check for upstream LLM API errors that generated the output being evaluated
        if llm_output.startswith("Error: LLM API call failed.") or \
           llm_output.startswith("Error: No content from LLM."):
            logger.warning(f"Agent '{self.agent_id}': Input LLM output indicates an API error: {llm_output}")
            return {
                "fitness_score": 0.0,
                "detailed_metrics": {"llm_call_status_upstream": "failed", "error_message_upstream": llm_output, "constraint_adherence_placeholder": 0.0},
                "error_analysis": [f"Input LLM generation failed: {llm_output}"]
            }

        logger.info(f"Agent '{self.agent_id}': Evaluating output for prompt: {str(prompt_chromosome)[:100]}... \nLLM Output to Evaluate: {llm_output[:100]}...")

        metrics = {}
        errors = [] # Errors from the evaluation process itself

        metrics.update(self._calculate_standard_metrics(llm_output, task_desc))
        
        constraint_feedback = self._check_constraints(llm_output, success_criteria)
        errors.extend(constraint_feedback.get("errors", [])) # Constraint violations are added to 'errors'
        metrics.update(constraint_feedback.get("metrics", {}))
        
        content_metrics, content_errors = self._analyze_content(llm_output, task_desc, prompt_chromosome)
        metrics.update(content_metrics)
        errors.extend(content_errors) # Errors from the LLM analysis process itself
        
        # Calculate overall fitness score
        constraint_adherence_score = metrics.get("constraint_adherence_placeholder", 0.0)
        llm_quality_assessment = metrics.get("llm_assessed_quality", 0.0)

        # Use weights from config
        weight_constraint = self.fitness_score_weights.get("constraint_adherence", 0.5)
        weight_quality = self.fitness_score_weights.get("llm_quality_assessment", 0.5)

        # If LLM evaluation itself failed, we might rely more on constraint adherence or basic placeholders
        if "LLM analysis not performed or failed" in metrics.get("llm_assessment_feedback", "") or \
           "Exception during LLM analysis" in metrics.get("llm_assessment_feedback", ""):
            logger.warning(f"Agent '{self.agent_id}': LLM quality assessment failed or was unreliable. Adjusting fitness calculation if necessary or relying on placeholders via llm_quality_assessment.")
            # llm_quality_assessment would be a random score from _analyze_content's fallback in this case.
            # Consider if weights should change or if this random score is acceptable.
            # For now, we use the (potentially random) llm_quality_assessment.

        fitness_score = (constraint_adherence_score * weight_constraint) + \
                        (llm_quality_assessment * weight_quality)
        
        # Further penalize if the LLM-based evaluation itself had errors (e.g., malformed JSON from eval LLM)
        # These 'content_errors' are about the evaluation process, not the content being evaluated.
        if content_errors:
            fitness_score -= len(content_errors) * 0.05 # Small penalty for each eval process issue
        
        fitness_score = max(0.0, min(1.0, fitness_score))

        logger.info(f"Agent '{self.agent_id}': Evaluation complete. Fitness: {fitness_score:.4f}, Metrics: {metrics}, Evaluation Process Errors: {len(content_errors)}, Constraint Violations: {len(constraint_feedback.get('errors',[]))}")
        return {
            "fitness_score": fitness_score,
            "detailed_metrics": metrics,
            "error_analysis": errors
        }

